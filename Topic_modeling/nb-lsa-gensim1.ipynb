{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: pip in c:\\users\\nb24634\\appdata\\local\\continuum\\miniconda3\\envs\\master_thesis\\lib\\site-packages (18.1)\n"
     ]
    }
   ],
   "source": [
    "# Commands to run\n",
    "\n",
    "#nltk.download('stopwords')\n",
    "#!pip install glove_python\n",
    "#!pip3 install glove\n",
    "#!python -m spacy download en # one time run\n",
    "#!python -m pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'glove'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-3eade42177ce>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mgensim\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mglove\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mGlove\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mglove\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mCorpus\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'glove'"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from glove import Glove\n",
    "from glove import Corpus\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import pyLDAvis.gensim\n",
    "import spacy\n",
    "import warnings\n",
    "from gensim.models import CoherenceModel, LdaModel, LsiModel, HdpModel, Phrases\n",
    "from gensim.models.wrappers import LdaMallet\n",
    "from gensim.corpora import Dictionary\n",
    "from nltk import FreqDist\n",
    "from nltk.corpus import stopwords\n",
    "from spacy.lang.en import English\n",
    "\n",
    "# fixed bars\n",
    "stop_words = stopwords.words('english')\n",
    "warnings.filterwarnings('ignore')  # Let's not pay heed to them right now\n",
    "nlp = spacy.load('en', disable=['parser', 'ner'])\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use this to read the sampled file\n",
    "df_sampled = pd.read_excel('excel_for_topic_modeling.xlsx', sheet_name=\"Sheet1\")\n",
    "df_sampled = df_sampled.sample(frac=0.25, random_state=1) #working with 0.01% of the total dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sampled.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to plot most frequent terms\n",
    "def freq_words(x, terms = 30):\n",
    "  all_words = ' '.join([text for text in x])\n",
    "  all_words = all_words.split()\n",
    "\n",
    "  fdist = FreqDist(all_words)\n",
    "  words_df = pd.DataFrame({'word':list(fdist.keys()), 'count':list(fdist.values())})\n",
    "\n",
    "  # selecting top 20 most frequent words\n",
    "  d = words_df.nlargest(columns=\"count\", n = terms) \n",
    "  plt.figure(figsize=(20,5))\n",
    "  ax = sns.barplot(data=d, x= \"word\", y = \"count\")\n",
    "  ax.set(ylabel = 'Count')\n",
    "  plt.show()\n",
    "    \n",
    "def remove_stopwords(rev):\n",
    "    rev_new = \" \".join([i for i in rev if i not in stop_words])\n",
    "    return rev_new\n",
    "\n",
    "def pos(texts, tags=['ADV', 'ADJ']): # filter noun and adjective\n",
    "    output = []\n",
    "    for sent in texts:\n",
    "         doc = nlp(\" \".join(sent)) \n",
    "         output.append([token.lemma_ for token in doc if token.pos_ in tags])\n",
    "    return output\n",
    "\n",
    "## Preprocessing ##\n",
    "\n",
    "def createUniqueText(clean_text):\n",
    "    text = \"\"\n",
    "    for sentence in clean_text:\n",
    "        for word in sentence:\n",
    "            text = text + ' ' + word\n",
    "        text = text + \"\\n\"\n",
    "    return text\n",
    "\n",
    "## Glove Functions ##\n",
    "                         \n",
    "def read_corpus(filename):\n",
    "    \"\"\"\n",
    "    Read corpus from regular text file\n",
    "    \"\"\"\n",
    "    delchars = [chr(c) for c in range(256)]\n",
    "    delchars = [x for x in delchars if not x.isalnum()]\n",
    "    delchars.remove(' ')\n",
    "    delchars = ' '.join(delchars)\n",
    "    table = str.maketrans(dict.fromkeys(delchars))\n",
    "    \n",
    "    with open(filename, 'r') as datafile:\n",
    "        for line in datafile:\n",
    "            yield line.lower().translate(table).split(' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove unwanted characters, numbers and symbols\n",
    "df_sampled['review'] = df_sampled['review'].str.replace(\"[^a-zA-Z#]\", \" \")\n",
    "\n",
    "#removing nan\n",
    "df_sampled.dropna()\n",
    "\n",
    "#convert everything to str\n",
    "df_sampled['review_modified'] = df_sampled['review'].astype(str)\n",
    "\n",
    "# remove short words (length < 3)\n",
    "df_sampled['review_modified'] = df_sampled['review_modified'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>3]))\n",
    "\n",
    "#Removing Stop Words\n",
    "df_sampled['review_modified'] = [remove_stopwords(r.split()) for r in df_sampled['review_modified']]\n",
    "\n",
    "#lower_case\n",
    "df_sampled['review_modified'] = [r.lower() for r in df_sampled['review_modified']]\n",
    "\n",
    "clean_text = df_sampled['review_modified'].apply(lambda x: x.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part of speech\n",
    "# text_pos retains all sentences(reviews) with tokens\n",
    "%time\n",
    "text_pos = pos(clean_text, tags=['ADV', 'ADJ'])\n",
    "\n",
    "for x in text_pos:\n",
    "    if 'alexa' in x:\n",
    "        x.remove(\"alexa\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stopwords because spacy better than nltk\n",
    "nlp = spacy.load('en')\n",
    "my_stop_words = [u'say', u'\\'s', u'Mr', u'be', u'said', u'says', u'saying']\n",
    "for stopword in my_stop_words:\n",
    "    lexeme = nlp.vocab[stopword]\n",
    "    lexeme.is_stop = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create unique text for gensim\n",
    "# since the file created is too big we need to split in half\n",
    "uniqueString = createUniqueText(text_pos)\n",
    "#len(uniqueString)\n",
    "firsthalf, secondhalf = uniqueString[:len(uniqueString)//2], uniqueString[len(uniqueString)//2:]\n",
    "#firsthalf\n",
    "\n",
    "length_string = len(string)\n",
    "    first_length = round(length_string / 2)\n",
    "    first_half = string[0:first_length].lower()\n",
    "    second_half = string[first_length:].upper()\n",
    "doc1 = nlp(uniqueString)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we add some words to the stop word list\n",
    "texts, article = [], []\n",
    "for w in doc:\n",
    "    # if it's not a stop word or punctuation mark, add it to our article!\n",
    "    if w.text != '\\n' and not w.is_stop and not w.is_punct and not w.like_num:\n",
    "        # we add the lematized version of the word\n",
    "        article.append(w.lemma_)\n",
    "    # if it's a new line, it means we're onto our next document\n",
    "    if w.text == '\\n':\n",
    "        texts.append(article)\n",
    "        article = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram = gensim.models.Phrases(texts)\n",
    "texts = [bigram[line] for line in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = Dictionary(texts)\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsimodel = LsiModel(corpus=corpus, num_topics=10, id2word=dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsimodel.show_topics(num_topics=5)  # Showing only the top 5 topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsitopics = [[word for word, prob in topic] for topicid, topic in lsimodel.show_topics(formatted=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Topic Coherence is a new gensim functionality where we can identify which topic model is 'better'. By returning a score, we can compare between different topic models of the same. \n",
    "# We use the same example from the news classification notebook to plot a graph between the topic models we have created.\n",
    "lsitopicsImportance = [[prob for word, prob in topic] for topicid, topic in lsimodel.show_topics(formatted=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TOPIC 1\n",
    "plt.figure(figsize=(15,10))\n",
    "plt.bar(lsitopics[0], lsitopicsImportance[0], align='center')\n",
    "plt.xlabel('Words')\n",
    "plt.ylabel('Importance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TOPIC 2\n",
    "plt.figure(figsize=(15,10))\n",
    "plt.bar(lsitopics[1], lsitopicsImportance[1], align='center')\n",
    "plt.xlabel('Words')\n",
    "plt.ylabel('Coherence Value')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TOPIC 3\n",
    "plt.figure(figsize=(15,10))\n",
    "plt.bar(lsitopics[2], lsitopicsImportance[2], align='center')\n",
    "plt.xlabel('Words')\n",
    "plt.ylabel('Coherence Value')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TOPIC 4\n",
    "plt.figure(figsize=(15,10))\n",
    "plt.bar(lsitopics[3], lsitopicsImportance[3], align='center')\n",
    "plt.xlabel('Words')\n",
    "plt.ylabel('Coherence Value')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TOPIC 5\n",
    "plt.figure(figsize=(15,10))\n",
    "plt.bar(lsitopics[4], lsitopicsImportance[4], align='center')\n",
    "plt.xlabel('Words')\n",
    "plt.ylabel('Coherence Value')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"glove_Textfile.txt\",\"w\")\n",
    "for setence in texts:\n",
    "    for word in setence:\n",
    "        file.write(word+'\\n')\n",
    "file.close()\n",
    "\n",
    "# get data from doc\n",
    "get_data = read_corpus('glove_Textfile.txt')\n",
    "\n",
    "corpus_model = Corpus()\n",
    "\n",
    "corpus_model.fit(get_data, window=10)\n",
    "\n",
    "epochs = 10\n",
    "no_threads = 8\n",
    "\n",
    "glove = Glove(no_components=100, learning_rate=0.05)\n",
    "glove.fit(corpus_model.matrix, epochs= epochs, no_threads=no_threads, verbose=True)\n",
    "glove.add_dictionary(corpus_model.dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_model.dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove.most_similar('safe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "master_thesis",
   "language": "python",
   "name": "master_thesis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
